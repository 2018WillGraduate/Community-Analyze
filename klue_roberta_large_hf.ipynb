{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPZcBqGIJTl",
        "outputId": "a297da0f-42d7-42bb-cfc8-b84d349e59f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ms8zpRlKNVj",
        "outputId": "cd8cc386-4199-4cee-f998-52113b50a621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive'\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htsIje03F3Y8",
        "outputId": "dab487a8-8990-414a-8ee2-1cdb153c1542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.23.5)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.32.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.4)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "356YR2SlGvPN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "Zc1rxm9pGxjo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data(gallery):\n",
        "  result = []\n",
        "  for name in os.listdir(path='/content/drive/MyDrive/jolnon/' + gallery):\n",
        "    with open('/content/drive/MyDrive/jolnon/' + gallery + '/' + name, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "      valid = []\n",
        "      for d in data['content'].split('\\n'):\n",
        "        res = []\n",
        "        for word in d.split(' '):\n",
        "          if cond.match(word) != None:\n",
        "            res.append(word)\n",
        "        d = ' '.join(res)\n",
        "        if d and not d.replace(' ', '').isdecimal():\n",
        "          valid.append(d)\n",
        "      result.append('\\n'.join([data['title']] + valid))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "7dpDEKYxbnxI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from zipfile import ZipFile\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data_from_zip(zip_name):\n",
        "    zip_file_path = '/content/drive/MyDrive/jolnon/' + zip_name + '.zip'\n",
        "    extract_path = '/content/' + zip_name\n",
        "\n",
        "    # zip 파일 압축 해제\n",
        "    with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    result = []\n",
        "    for name in os.listdir(path=extract_path):\n",
        "        file_path = os.path.join(extract_path, name)\n",
        "        if os.path.isdir(file_path):\n",
        "            for name in os.listdir(path=file_path):\n",
        "                with open(file_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    valid = []\n",
        "                    for d in data['content'].split('\\n'):\n",
        "                        res = []\n",
        "                        for word in d.split(' '):\n",
        "                            if cond.match(word) != None:\n",
        "                                res.append(word)\n",
        "                        d = ' '.join(res)\n",
        "                        if d and not d.replace(' ', '').isdecimal():\n",
        "                            valid.append(d)\n",
        "                    result.append('\\n'.join([data['title']] + valid))\n",
        "        else:\n",
        "            with open(extract_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                valid = []\n",
        "                for d in data['content'].split('\\n'):\n",
        "                    res = []\n",
        "                    for word in d.split(' '):\n",
        "                        if cond.match(word) != None:\n",
        "                            res.append(word)\n",
        "                    d = ' '.join(res)\n",
        "                    if d and not d.replace(' ', '').isdecimal():\n",
        "                        valid.append(d)\n",
        "                result.append('\\n'.join([data['title']] + valid))\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "ATei8ItuKmZF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "galleries = ['giants_new2','cock_tail']#,'ktwiz','kartriderdrift','skwyverns_new1','ncdinos','samsunglions_new','doosanbears_new1','giants_new2','sh_new','lgtwins_new','tigers_new']\n",
        "dataset = []\n",
        "for e in galleries:\n",
        "  dataset.append([get_data_from_zip(e)])\n",
        "cocktail = []\n",
        "for data in dataset:\n",
        "  for sen in data:\n",
        "    cocktail+=sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "5K6H00ybG72L"
      },
      "outputs": [],
      "source": [
        "def remove_words(sentence, word):\n",
        "    return ' '.join([w for w in sentence.split(' ') if word not in w.lower()])\n",
        "stopwords = []#['ㅋㅋ', 'ex', '나는', 'on', '이미지', '순서', '오늘', '일단', 'and', '이야', '그리고', '내일', '그냥', '000', '조금', '살짝', 'ㅇㅇ', 'ㅈㄱㄴ', '있음', '이거', '내가', '칵하하하', '칵하핫', '근데', '지듣노', 'youtube ', '야스중', '우흥', '한다', 'ㅎㅎ', 'ㅠㅠ', '로오오오오오옹', '하고', '아침', '것도', '추천', '혹시', '새낀데', '같다']\n",
        "for i in range(len(cocktail)):\n",
        "  for j in range(len(stopwords)):\n",
        "    cocktail[i] = remove_words(cocktail[i],stopwords[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "id": "RN4RVv0XVn9S"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "def remove_top_n_words(data, n):\n",
        "    words = [word for sentence in data for word in sentence.split(' ')]\n",
        "    word_count = collections.Counter(words)\n",
        "    top_n_words = [word for word, _ in word_count.most_common(n)]\n",
        "    for i in range(len(data)):\n",
        "        for word in top_n_words:\n",
        "            if word=='':\n",
        "                continue\n",
        "            data[i] = remove_words(data[i], word)\n",
        "    return data\n",
        "\n",
        "def remove_empty_sentences(cocktail, real_labels):\n",
        "  indices = [i for i, sentence in enumerate(cocktail) if sentence.strip() != '']\n",
        "  cocktail = [cocktail[i] for i in indices]\n",
        "  real_labels = [real_labels[i] for i in indices]\n",
        "  return cocktail, real_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "id": "7z1QOLSbkE8Q"
      },
      "outputs": [],
      "source": [
        "real_labels = []\n",
        "for i in range(len(dataset)):\n",
        "  real_labels += [i] * len(dataset[i][0])\n",
        "cocktail = remove_top_n_words(cocktail, 0)\n",
        "cocktail, real_labels = remove_empty_sentences(cocktail, real_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5H7fM7WtMH3",
        "outputId": "f7821ad9-3da6-44ee-cde9-c0bf1718a9d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from umap import UMAP\n",
        "\n",
        "# umap_model = UMAP(random_state=42)\n",
        "# umap_model = UMAP(n_neighbors=15, n_components=10, metric='cosine', random_state=42)\n",
        "model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "syQeD8HxLW0G"
      },
      "outputs": [],
      "source": [
        "# bertopic_model = BERTopic(language='multilingual',\n",
        "#                           # umap_model=umap_model,\n",
        "#                           nr_topics=100,\n",
        "#                           top_n_words=1)\n",
        "# topics, probs = bertopic_model.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "Vy6m2m8PLs3a"
      },
      "outputs": [],
      "source": [
        "# bertopic_model.visualize_topics()\n",
        "# bertopic_model.visualize_distribution(probs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "07LnTqhNJ58P"
      },
      "outputs": [],
      "source": [
        "# from transformers import RobertaModel, RobertaTokenizer\n",
        "# import torch\n",
        "\n",
        "# model = RobertaModel.from_pretrained(\"klue/roberta-large\")\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"klue/roberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "a25-3DPEY6fG"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "  # def __call__(self, target):\n",
        "  #   return self.tokenizer.tokenize(target)\n",
        "  def __call__(self, target):\n",
        "    tokens = self.tokenizer.tokenize(target)\n",
        "    if isinstance(tokens[0], str):\n",
        "      indices = [i for i, token in enumerate(tokens) if not token.startswith('##')]\n",
        "      tokens = [tokens[i] for i in indices]\n",
        "    elif isinstance(tokens[0][0], str):\n",
        "      for j in range(len(tokens)):\n",
        "        indices = [i for i, token in enumerate(tokens[j]) if not token.startswith('##')]\n",
        "        tokens[j] = [tokens[j][i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "7tUS_OcvLAdl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "custom_tokenizer = CustomTokenizer(tokenizer)\n",
        "# vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)\n",
        "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "HjjHwLCltYbv"
      },
      "outputs": [],
      "source": [
        "# bertopic_model_embedding = BERTopic(embedding_model=model,\n",
        "#                                     # umap_model=umap_model,\n",
        "#                                     nr_topics=100,\n",
        "#                                     top_n_words=1)\n",
        "# topics, probs = bertopic_model_embedding.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "znUvBHBMtc3w"
      },
      "outputs": [],
      "source": [
        "# bertopic_model_tokenizer = BERTopic(vectorizer_model=vectorizer,\n",
        "#                                     # umap_model=umap_model,\n",
        "#                                     nr_topics=100,\n",
        "#                                     top_n_words=1)\n",
        "# topics, _ = bertopic_model_tokenizer.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHV1Xm6btyCQ",
        "outputId": "976d148a-09f7-48a1-93df-ef6a5ce9951d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (61889 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "bertopic_model_embed_token = BERTopic(embedding_model=model,\n",
        "                                      # umap_model=umap_model,\n",
        "                                      vectorizer_model=vectorizer,\n",
        "                                      nr_topics=100,\n",
        "                                      top_n_words=3)\n",
        "topics, _ = bertopic_model_embed_token.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMZIXrc1G3hs",
        "outputId": "177b4fd9-98de-4bc8-b20c-d2b1043ba1f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "생성된 주제의 수: 71\n",
            "뿌_딱_아깝\n",
            "릴_내_베르\n",
            "ㄴ_^_ㄱ\n",
            "a_강민호_o\n",
            "+_❤_근\n",
            "j_현역_통산\n",
            "생과_인_벌써\n",
            "c_앱_질렀\n",
            "봐라_병신_이거\n",
            "vs_칵_핫\n",
            "9_10_화력\n",
            "7_5_2\n",
            "쓱_싸_쓰레기\n",
            "송도_인천_도시\n",
            "손주_날려_좌회전\n",
            "경상도_뒤통수_베일\n",
            "털어_,_서브\n",
            "검지_방금_버스\n",
            "동맹_갸_홍\n",
            "끓인_입_오늘\n",
            "등록_구분_쿠폰\n",
            "나부_하자_시작\n",
            "용_여긴_조기\n",
            "그란_자이언츠_니트\n",
            "쥐_근무_우리\n",
            "인증_에비_연락\n",
            "릴_받_이미지\n",
            "리필_스트로_피츠\n",
            "·_다이_09\n",
            "바질_이미지_순서\n",
            "시대_범위_친구\n",
            "시그_._산다\n",
            "입_시험_압\n",
            "불꽃_진지_뿌려\n",
            "롬_~_최강\n",
            "홈런_?_함\n",
            "지리_임마_아들\n",
            "할머니_달린다_자\n",
            "시작_야스_황성\n",
            "에반_윌리엄스_릴\n",
            "나눔_만_비트\n",
            "나눔_깜_소독\n",
            "나눔_받_키위\n",
            "붙_역_쉐\n",
            "소다_베르_릴\n",
            "단_목_돈\n",
            "어제_떴_센\n",
            "끊_공간_앞\n",
            "모집_아니야_비호\n",
            "생수_<_과학\n",
            "워_~_뭐\n",
            "능력_슬슬_유치\n",
            "뭐_아무_시키\n",
            "건강_낮잠_선물\n",
            "곰팡이_견과_종\n",
            "소금_생겼_병\n",
            "통풍_플로_대통령\n",
            "쳐_박세_최악\n",
            "한다_결과물_올라온\n",
            "선생_터뜨리_인정\n",
            "합격_옮기_라인업\n",
            "모스_믹스_헤이\n",
            "?_최강_하나\n",
            "빨대_실수_코스\n",
            "존나_매년_바를\n",
            "곤_받_드\n",
            "코코넛_파인애플_그래\n",
            "서_천_<\n",
            "안치_농협_노무현\n",
            "아닐까_화이트_다크\n",
            "플_생크림_오크\n"
          ]
        }
      ],
      "source": [
        "topic_info = bertopic_model_embed_token.get_topic_info()\n",
        "number_of_topics = len(topic_info) - 1\n",
        "print(f\"생성된 주제의 수: {number_of_topics}\")\n",
        "lst = []\n",
        "for index, row in topic_info.iterrows():\n",
        "    topic_num = row['Topic']\n",
        "    if topic_num != -1:\n",
        "        topic_name = row['Name']\n",
        "        pure_topic_name = \"_\".join(topic_name.split(\"_\")[1:])\n",
        "        lst.append(pure_topic_name)\n",
        "for i in lst:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "id": "7Uruk1MXQjY-"
      },
      "outputs": [],
      "source": [
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "bertopic_model_embed_token = BERTopic(embedding_model=model,\n",
        "                                      # umap_model=umap_model,\n",
        "                                      vectorizer_model=vectorizer,\n",
        "                                      ctfidf_model=ctfidf_model,\n",
        "                                      nr_topics=100,\n",
        "                                      top_n_words=3,\n",
        "                                      calculate_probabilities=True)\n",
        "topics, _ = bertopic_model_embed_token.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7QOVUY2QuUI",
        "outputId": "7a43c9ba-b685-458b-d96a-4d9709ed9a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "생성된 주제의 수: 72\n",
            "뿌_아깝_취한다\n",
            "호두_귀_삶\n",
            "a_강민호_o\n",
            "ㄴ_^_엌\n",
            "+_❤_근\n",
            "j_통산_현역\n",
            "생과_인_비싸\n",
            "c_질렀_멤버십\n",
            "vs_빨대_핫\n",
            "봐라_템_5000\n",
            "7_5_700\n",
            "9_10_화력\n",
            "쓱_싸_쓰레기\n",
            "천하_기운_권\n",
            "끓인_뜻_청량\n",
            "송도_우리나라_쇼핑\n",
            "손주_날려_좌회전\n",
            "경상도_뒤통수_바다\n",
            "털어_서브_글라스\n",
            "나부_하자_@\n",
            "여긴_조기_절반\n",
            "면세_시대_범위\n",
            "엮_옮겨_조회\n",
            "구분_등록_을\n",
            "윌리엄스_에반_려\n",
            "바질_타임_사마\n",
            "어이_오일_쪼\n",
            "홈런_이택_삭제\n",
            "롬_사이드_완료\n",
            "은퇴_화장실_연락\n",
            "그란_자이언츠_”\n",
            "붙_역_밝\n",
            "끊_앞_옥수수\n",
            "모집_고백_과제\n",
            "소독_깜_독\n",
            "야스_부스_수확\n",
            "치우_근무_찾아\n",
            "18_잤_땜\n",
            "지리_임마_아들\n",
            "비트_맨해튼_만\n",
            "키위_나눔_대학생\n",
            "소다_베르_케\n",
            "·_09_가리\n",
            "스트로_리필_피츠\n",
            "불꽃_뿌려_엑스포\n",
            "목_무시_잠들\n",
            "센_떴_구장\n",
            "능력_다르_유치\n",
            "생수_셀_가치\n",
            "구도_가장_익숙\n",
            "최정_이대호_오해\n",
            "원두_뭔가_골드\n",
            "워_이진_관\n",
            "고장_냉동실_동네\n",
            "신청_망하_파도\n",
            "감사_생겼_관중\n",
            "오타_그랬_정도\n",
            "한다_결과물_올라온\n",
            "실수_김해_제출\n",
            "선발투수_펜_통풍\n",
            "매년_자기_시가\n",
            "바를_진로_대단\n",
            "플레_견과_곰팡이\n",
            "부심_신인_맹\n",
            "금액_굿_중요\n",
            "상큼_검색_흐르\n",
            "품절_생크림_카카오\n",
            "에_멘_메\n",
            "모스_헤이_탓\n",
            "분류_곤_왜냐면\n",
            "옮기_합격_연기\n",
            "안치_응_킹\n"
          ]
        }
      ],
      "source": [
        "topic_info = bertopic_model_embed_token.get_topic_info()\n",
        "number_of_topics = len(topic_info) - 1\n",
        "print(f\"생성된 주제의 수: {number_of_topics}\")\n",
        "lst = []\n",
        "for index, row in topic_info.iterrows():\n",
        "    topic_num = row['Topic']\n",
        "    if topic_num != -1:\n",
        "        topic_name = row['Name']\n",
        "        pure_topic_name = \"_\".join(topic_name.split(\"_\")[1:])\n",
        "        lst.append(pure_topic_name)\n",
        "for i in lst:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4BL4YWQmyBS",
        "outputId": "ec053553-e3bf-4519-cf01-c5833dc157b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[14, 6], [1, 25], [15, 1], [16, 8], [18, 0], [54, 1], [50, 117], [0, 13], [3, 67], [14, 4], [24, 10], [24, 7], [23, 20], [7, 4], [0, 13], [15, 0], [27, 32], [17, 22], [85, 151], [1, 11], [17, 7], [16, 8], [13, 7], [2, 11], [1, 19], [0, 14], [3, 44], [24, 20], [10, 18], [43, 9], [13, 10], [12, 5], [15, 9], [19, 9], [3, 20], [6, 18], [6, 4], [5, 8], [24, 7], [1, 34], [3, 77], [4, 20], [3, 19], [1, 16], [19, 38], [10, 14], [16, 13], [3, 9], [6, 6], [9, 9], [4, 9], [5, 6], [22, 42], [5, 8], [43, 38], [11, 11], [41, 66], [12, 12], [28, 22], [18, 24], [11, 7], [6, 7], [16, 8], [5, 5], [3, 27], [2, 9], [2, 33], [2, 12], [1, 9], [3, 46], [2, 10], [53, 68]]\n"
          ]
        }
      ],
      "source": [
        "table = [[0 for r in range(2)] for t in range(number_of_topics)]\n",
        "\n",
        "for j in range(number_of_topics):\n",
        "  for i in range(len(topics)):\n",
        "    if topics[i]==j:\n",
        "      table[j][real_labels[i]]+=1\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Reducing outliers\n",
        "# topics = bertopic_model_embed_token.reduce_outliers(cocktail, topics, strategy=\"c-tf-idf\", threshold=0.1)\n",
        "# topics = bertopic_model_embed_token.reduce_outliers(cocktail, topics, strategy=\"distributions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = [\"#0000FF\", \"#FF0000\"]\n",
        "\n",
        "data = [[] for _ in range(len(dataset))]\n",
        "for j in range(len(dataset)):\n",
        "    data[j] = [topics[i] for i in range(len(real_labels)) if real_labels[i] == j]\n",
        "    plt.hist(data[j], bins=number_of_topics, color=colors[j], histtype='bar')\n",
        "    plt.show()\n",
        "\n",
        "plt.hist(data, bins=number_of_topics, color=colors, histtype='bar', density=True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
