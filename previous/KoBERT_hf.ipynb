{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPZcBqGIJTl",
        "outputId": "ac8fe8d5-c269-46df-fbb3-bd6904361ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ms8zpRlKNVj",
        "outputId": "e6683fd6-05e4-4821-de0d-4fea0e7fab02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htsIje03F3Y8",
        "outputId": "00ad10df-b01a-4db3-c3c0-22f4f9bed373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.22.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.65.0)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.13.1)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.31.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.4)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5-_SutcF8ar",
        "outputId": "eb0216cd-a675-484b-8e08-0e64344276e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-v4pj4u24/kobert-tokenizer_20bfc3c2551f4c6b94a78b1c314f812b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-v4pj4u24/kobert-tokenizer_20bfc3c2551f4c6b94a78b1c314f812b\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "356YR2SlGvPN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Zc1rxm9pGxjo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data(gallery):\n",
        "  result = []\n",
        "  for name in os.listdir(path='/content/drive/MyDrive/jolnon/' + gallery):\n",
        "    with open('/content/drive/MyDrive/jolnon/' + gallery + '/' + name, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "      valid = []\n",
        "      for d in data['content'].split('\\n'):\n",
        "        res = []\n",
        "        for word in d.split(' '):\n",
        "          if cond.match(word) != None:\n",
        "            res.append(word)\n",
        "        d = ' '.join(res)\n",
        "        if d and not d.replace(' ', '').isdecimal():\n",
        "          valid.append(d)\n",
        "      result.append('\\n'.join([data['title']] + valid))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from zipfile import ZipFile\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data_from_zip(zip_name):\n",
        "    zip_file_path = '/content/drive/MyDrive/jolnon/' + zip_name + '.zip'\n",
        "    extract_path = '/content/' + zip_name\n",
        "\n",
        "    # zip 파일 압축 해제\n",
        "    with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    result = []\n",
        "    for name in os.listdir(path=extract_path):\n",
        "        file_path = os.path.join(extract_path, name)\n",
        "        if os.path.isdir(file_path):\n",
        "            for name in os.listdir(path=file_path):\n",
        "                with open(file_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    valid = []\n",
        "                    for d in data['content'].split('\\n'):\n",
        "                        res = []\n",
        "                        for word in d.split(' '):\n",
        "                            if cond.match(word) != None:\n",
        "                                res.append(word)\n",
        "                        d = ' '.join(res)\n",
        "                        if d and not d.replace(' ', '').isdecimal():\n",
        "                            valid.append(d)\n",
        "                    result.append('\\n'.join([data['title']] + valid))\n",
        "        else:\n",
        "            with open(extract_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                valid = []\n",
        "                for d in data['content'].split('\\n'):\n",
        "                    res = []\n",
        "                    for word in d.split(' '):\n",
        "                        if cond.match(word) != None:\n",
        "                            res.append(word)\n",
        "                    d = ' '.join(res)\n",
        "                    if d and not d.replace(' ', '').isdecimal():\n",
        "                        valid.append(d)\n",
        "                result.append('\\n'.join([data['title']] + valid))\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "7dpDEKYxbnxI"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "ATei8ItuKmZF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "galleries = ['giants_new2','cock_tail']#,'ktwiz','kartriderdrift','skwyverns_new1','ncdinos','samsunglions_new','doosanbears_new1','giants_new2','sh_new','lgtwins_new','tigers_new']\n",
        "dataset = []\n",
        "for e in galleries:\n",
        "  dataset.append([get_data_from_zip(e)])\n",
        "cocktail = []\n",
        "for data in dataset:\n",
        "  for sen in data:\n",
        "    cocktail+=sen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words(sentence, word):\n",
        "    return ' '.join([w for w in sentence.split(' ') if word not in w.lower()])\n",
        "stopwords = ['ㅋㅋ', 'ex', '나는', 'on', '이미지', '순서', '오늘', '일단', 'and', '이야', '그리고', '내일', '그냥', '000', '조금', '살짝', 'ㅇㅇ', 'ㅈㄱㄴ', '있음', '이거', '내가', '칵하하하', '칵하핫', '근데', '지듣노', 'youtube ', '야스중', '우흥', '한다', 'ㅎㅎ', 'ㅠㅠ', '로오오오오오옹', '하고', '아침', '것도', '추천', '혹시', '새낀데', '같다']\n",
        "for i in range(len(cocktail)):\n",
        "  for j in range(len(stopwords)):\n",
        "    cocktail[i] = remove_words(cocktail[i],stopwords[j])"
      ],
      "metadata": {
        "id": "5K6H00ybG72L"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def remove_top_n_words(data, n):\n",
        "    words = [word for sentence in data for word in sentence.split(' ')]\n",
        "    word_count = collections.Counter(words)\n",
        "    top_n_words = [word for word, _ in word_count.most_common(n)]\n",
        "    for i in range(len(data)):\n",
        "        for word in top_n_words:\n",
        "            if word=='':\n",
        "                continue\n",
        "            data[i] = remove_words(data[i], word)\n",
        "    return data\n",
        "\n",
        "def remove_empty_sentences(cocktail, real_labels):\n",
        "  indices = [i for i, sentence in enumerate(cocktail) if sentence.strip() != '']\n",
        "  cocktail = [cocktail[i] for i in indices]\n",
        "  real_labels = [real_labels[i] for i in indices]\n",
        "  return cocktail, real_labels\n"
      ],
      "metadata": {
        "id": "RN4RVv0XVn9S"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_labels = []\n",
        "for i in range(len(dataset)):\n",
        "  real_labels += [i] * len(dataset[i][0])\n",
        "cocktail = remove_top_n_words(cocktail, 20)\n",
        "cocktail, real_labels = remove_empty_sentences(cocktail, real_labels)"
      ],
      "metadata": {
        "id": "7z1QOLSbkE8Q"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calc_purity(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    _, col_ind = linear_sum_assignment(cm, maximize=True)\n",
        "    reordered_cm = cm[:, col_ind]\n",
        "    purity = np.sum(np.max(reordered_cm, axis=0)) / np.sum(reordered_cm)\n",
        "    return purity, reordered_cm"
      ],
      "metadata": {
        "id": "Ov862AN8vkG3"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "syQeD8HxLW0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41af2ad-84fb-425e-816d-f9be41f2913b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4789\n",
            "[[ 881 1182]\n",
            " [1232 1494]]\n",
            "Purity:  0.569221131760284\n",
            "Reordered Confusion Matrix: \n",
            " [[1182  881]\n",
            " [1494 1232]]\n"
          ]
        }
      ],
      "source": [
        "bertopic_model = BERTopic(language='multilingual',\n",
        "                          nr_topics='auto',\n",
        "                          top_n_words=3,\n",
        "                          calculate_probabilities=True)\n",
        "topics, probs = bertopic_model.fit_transform(cocktail)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "n_clusters = len(dataset)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(probs)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(len(real_labels))\n",
        "print(confusion_matrix(real_labels, labels))\n",
        "\n",
        "purity, reordered_cm = calc_purity(real_labels, labels)\n",
        "print(\"Purity: \", purity)\n",
        "print(\"Reordered Confusion Matrix: \\n\", reordered_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Vy6m2m8PLs3a"
      },
      "outputs": [],
      "source": [
        "# bertopic_model.visualize_topics()\n",
        "# bertopic_model.visualize_distribution(probs[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tokenizer.encode(\"한국어 모델을 공유합니다.\")\n",
        "\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "import torch\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5H7fM7WtMH3",
        "outputId": "49097fff-611d-44b0-b87e-d510bd48f373"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-l5edaxni/kobert-tokenizer_7917b72dcc274314b4f4e90ec86315fc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-l5edaxni/kobert-tokenizer_7917b72dcc274314b4f4e90ec86315fc\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertopic_model_embedding = BERTopic(embedding_model=model,\n",
        "                                    nr_topics='auto',\n",
        "                                    top_n_words=3,\n",
        "                                    calculate_probabilities=True)\n",
        "topics, probs = bertopic_model_embedding.fit_transform(cocktail)\n",
        "n_clusters = len(dataset)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(probs)\n",
        "labels = kmeans.labels_\n",
        "print(len(real_labels))\n",
        "print(confusion_matrix(real_labels, labels))\n",
        "\n",
        "purity, reordered_cm = calc_purity(real_labels, labels)\n",
        "print(\"Purity: \", purity)\n",
        "print(\"Reordered Confusion Matrix: \\n\", reordered_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjjHwLCltYbv",
        "outputId": "ec62c445-8500-437e-b6fd-be7ed76b4fd1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4789\n",
            "[[1935  128]\n",
            " [2590  136]]\n",
            "Purity:  0.569221131760284\n",
            "Reordered Confusion Matrix: \n",
            " [[ 128 1935]\n",
            " [ 136 2590]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "  def __call__(self, target):\n",
        "    return self.tokenizer.tokenize(target)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "custom_tokenizer = CustomTokenizer(tokenizer)\n",
        "# vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)\n",
        "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=3000)\n",
        "\n",
        "bertopic_model_tokenizer = BERTopic(vectorizer_model=vectorizer,\n",
        "                                    nr_topics='auto',\n",
        "                                    top_n_words=3,\n",
        "                                    calculate_probabilities=True)\n",
        "topics, probs = bertopic_model_tokenizer.fit_transform(cocktail)\n",
        "n_clusters = len(dataset)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(probs)\n",
        "labels = kmeans.labels_\n",
        "print(len(real_labels))\n",
        "print(confusion_matrix(real_labels, labels))\n",
        "\n",
        "purity, reordered_cm = calc_purity(real_labels, labels)\n",
        "print(\"Purity: \", purity)\n",
        "print(\"Reordered Confusion Matrix: \\n\", reordered_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znUvBHBMtc3w",
        "outputId": "9f753e95-1133-4dce-b2d5-b0ca91e46a1a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4789\n",
            "[[1264  799]\n",
            " [1751  975]]\n",
            "Purity:  0.569221131760284\n",
            "Reordered Confusion Matrix: \n",
            " [[ 799 1264]\n",
            " [ 975 1751]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bertopic_model_embed_token = BERTopic(embedding_model=model,\n",
        "                                      vectorizer_model=vectorizer,\n",
        "                                      nr_topics='auto',\n",
        "                                      top_n_words=3,\n",
        "                                      calculate_probabilities=True)\n",
        "topics, probs = bertopic_model_embed_token.fit_transform(cocktail)\n",
        "n_clusters = len(dataset)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "kmeans.fit(probs)\n",
        "labels = kmeans.labels_\n",
        "print(len(real_labels))\n",
        "print(confusion_matrix(real_labels, labels))\n",
        "\n",
        "purity, reordered_cm = calc_purity(real_labels, labels)\n",
        "print(\"Purity: \", purity)\n",
        "print(\"Reordered Confusion Matrix: \\n\", reordered_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHV1Xm6btyCQ",
        "outputId": "23b2c232-04b3-428e-c9ca-e44b9bd51b01"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4789\n",
            "[[1946  117]\n",
            " [2595  131]]\n",
            "Purity:  0.569221131760284\n",
            "Reordered Confusion Matrix: \n",
            " [[ 117 1946]\n",
            " [ 131 2595]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}