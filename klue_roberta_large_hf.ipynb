{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPZcBqGIJTl",
        "outputId": "a0e540fc-d6ef-4960-bb34-0a7b18acb6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ms8zpRlKNVj",
        "outputId": "ffc4a9f2-f2de-4f16-aacd-41a979a68e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htsIje03F3Y8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "356YR2SlGvPN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "Zc1rxm9pGxjo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data(gallery):\n",
        "  result = []\n",
        "  for name in os.listdir(path='/content/drive/MyDrive/jolnon/' + gallery):\n",
        "    with open('/content/drive/MyDrive/jolnon/' + gallery + '/' + name, 'r', encoding='utf-8') as f:\n",
        "      data = json.load(f)\n",
        "      valid = []\n",
        "      for d in data['content'].split('\\n'):\n",
        "        res = []\n",
        "        for word in d.split(' '):\n",
        "          if cond.match(word) != None:\n",
        "            res.append(word)\n",
        "        d = ' '.join(res)\n",
        "        if d and not d.replace(' ', '').isdecimal():\n",
        "          valid.append(d)\n",
        "      result.append('\\n'.join([data['title']] + valid))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from zipfile import ZipFile\n",
        "\n",
        "cond = re.compile('[가-힣]+')\n",
        "\n",
        "def get_data_from_zip(zip_name):\n",
        "    zip_file_path = '/content/drive/MyDrive/jolnon/' + zip_name + '.zip'\n",
        "    extract_path = '/content/' + zip_name\n",
        "\n",
        "    # zip 파일 압축 해제\n",
        "    with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    result = []\n",
        "    for name in os.listdir(path=extract_path):\n",
        "        file_path = os.path.join(extract_path, name)\n",
        "        if os.path.isdir(file_path):\n",
        "            for name in os.listdir(path=file_path):\n",
        "                with open(file_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    valid = []\n",
        "                    for d in data['content'].split('\\n'):\n",
        "                        res = []\n",
        "                        for word in d.split(' '):\n",
        "                            if cond.match(word) != None:\n",
        "                                res.append(word)\n",
        "                        d = ' '.join(res)\n",
        "                        if d and not d.replace(' ', '').isdecimal():\n",
        "                            valid.append(d)\n",
        "                    result.append('\\n'.join([data['title']] + valid))\n",
        "        else:\n",
        "            with open(extract_path + '/' + name, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                valid = []\n",
        "                for d in data['content'].split('\\n'):\n",
        "                    res = []\n",
        "                    for word in d.split(' '):\n",
        "                        if cond.match(word) != None:\n",
        "                            res.append(word)\n",
        "                    d = ' '.join(res)\n",
        "                    if d and not d.replace(' ', '').isdecimal():\n",
        "                        valid.append(d)\n",
        "                result.append('\\n'.join([data['title']] + valid))\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "7dpDEKYxbnxI"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "ATei8ItuKmZF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "galleries = ['giants_new2','cock_tail']#,'ktwiz','kartriderdrift','skwyverns_new1','ncdinos','samsunglions_new','doosanbears_new1','giants_new2','sh_new','lgtwins_new','tigers_new']\n",
        "dataset = []\n",
        "for e in galleries:\n",
        "  dataset.append([get_data_from_zip(e)])\n",
        "cocktail = []\n",
        "for data in dataset:\n",
        "  for sen in data:\n",
        "    cocktail+=sen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words(sentence, word):\n",
        "    return ' '.join([w for w in sentence.split(' ') if word not in w.lower()])\n",
        "stopwords = []#['ㅋㅋ', 'ex', '나는', 'on', '이미지', '순서', '오늘', '일단', 'and', '이야', '그리고', '내일', '그냥', '000', '조금', '살짝', 'ㅇㅇ', 'ㅈㄱㄴ', '있음', '이거', '내가', '칵하하하', '칵하핫', '근데', '지듣노', 'youtube ', '야스중', '우흥', '한다', 'ㅎㅎ', 'ㅠㅠ', '로오오오오오옹', '하고', '아침', '것도', '추천', '혹시', '새낀데', '같다']\n",
        "for i in range(len(cocktail)):\n",
        "  for j in range(len(stopwords)):\n",
        "    cocktail[i] = remove_words(cocktail[i],stopwords[j])"
      ],
      "metadata": {
        "id": "5K6H00ybG72L"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def remove_top_n_words(data, n):\n",
        "    words = [word for sentence in data for word in sentence.split(' ')]\n",
        "    word_count = collections.Counter(words)\n",
        "    top_n_words = [word for word, _ in word_count.most_common(n)]\n",
        "    for i in range(len(data)):\n",
        "        for word in top_n_words:\n",
        "            if word=='':\n",
        "                continue\n",
        "            data[i] = remove_words(data[i], word)\n",
        "    return data\n",
        "\n",
        "def remove_empty_sentences(cocktail, real_labels):\n",
        "  indices = [i for i, sentence in enumerate(cocktail) if sentence.strip() != '']\n",
        "  cocktail = [cocktail[i] for i in indices]\n",
        "  real_labels = [real_labels[i] for i in indices]\n",
        "  return cocktail, real_labels\n"
      ],
      "metadata": {
        "id": "RN4RVv0XVn9S"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_labels = []\n",
        "for i in range(len(dataset)):\n",
        "  real_labels += [i] * len(dataset[i][0])\n",
        "cocktail = remove_top_n_words(cocktail, 0)\n",
        "cocktail, real_labels = remove_empty_sentences(cocktail, real_labels)"
      ],
      "metadata": {
        "id": "7z1QOLSbkE8Q"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from umap import UMAP\n",
        "\n",
        "umap_model = UMAP(random_state=42)\n",
        "model = AutoModel.from_pretrained(\"klue/roberta-large\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5H7fM7WtMH3",
        "outputId": "face9954-7f8c-42eb-8fa3-9743d181c9b9"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "syQeD8HxLW0G"
      },
      "outputs": [],
      "source": [
        "# bertopic_model = BERTopic(language='multilingual',\n",
        "#                           umap_model=umap_model,\n",
        "#                           nr_topics=100,\n",
        "#                           top_n_words=1)\n",
        "# topics, probs = bertopic_model.fit_transform(cocktail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Vy6m2m8PLs3a"
      },
      "outputs": [],
      "source": [
        "# bertopic_model.visualize_topics()\n",
        "# bertopic_model.visualize_distribution(probs[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import RobertaModel, RobertaTokenizer\n",
        "# import torch\n",
        "\n",
        "# model = RobertaModel.from_pretrained(\"klue/roberta-large\")\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"klue/roberta-large\")"
      ],
      "metadata": {
        "id": "07LnTqhNJ58P"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "  def __call__(self, target):\n",
        "    return self.tokenizer.tokenize(target)\n",
        "  # def __call__(self, target):\n",
        "  #   tokens = self.tokenizer.tokenize(target)\n",
        "  #   if isinstance(tokens[0], str):\n",
        "  #     indices = [i for i, token in enumerate(tokens) if not token.startswith('##')]\n",
        "  #     tokens = [tokens[i] for i in indices]\n",
        "  #   elif isinstance(tokens[0][0], str):\n",
        "  #     for j in range(len(tokens)):\n",
        "  #       indices = [i for i, token in enumerate(tokens[j]) if not token.startswith('##')]\n",
        "  #       tokens[j] = [tokens[j][i] for i in indices]\n",
        "  #   return tokens"
      ],
      "metadata": {
        "id": "a25-3DPEY6fG"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "custom_tokenizer = CustomTokenizer(tokenizer)\n",
        "# vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=3000)\n",
        "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_features=3000)"
      ],
      "metadata": {
        "id": "7tUS_OcvLAdl"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bertopic_model_embedding = BERTopic(embedding_model=model,\n",
        "#                                     umap_model=umap_model,\n",
        "#                                     nr_topics=100,\n",
        "#                                     top_n_words=1)\n",
        "# topics, probs = bertopic_model_embedding.fit_transform(cocktail)"
      ],
      "metadata": {
        "id": "HjjHwLCltYbv"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bertopic_model_tokenizer = BERTopic(vectorizer_model=vectorizer,\n",
        "#                                     umap_model=umap_model,\n",
        "#                                     nr_topics=100,\n",
        "#                                     top_n_words=1)\n",
        "# topics, _ = bertopic_model_tokenizer.fit_transform(cocktail)"
      ],
      "metadata": {
        "id": "znUvBHBMtc3w"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertopic_model_embed_token = BERTopic(embedding_model=model,\n",
        "                                      umap_model=umap_model,\n",
        "                                      vectorizer_model=vectorizer,\n",
        "                                      nr_topics=100,\n",
        "                                      top_n_words=3)\n",
        "topics, _ = bertopic_model_embed_token.fit_transform(cocktail)"
      ],
      "metadata": {
        "id": "CHV1Xm6btyCQ"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = bertopic_model_embed_token.get_topic_info()\n",
        "number_of_topics = len(topic_info) - 1\n",
        "print(f\"생성된 주제의 수: {number_of_topics}\")\n",
        "lst = []\n",
        "for index, row in topic_info.iterrows():\n",
        "    topic_num = row['Topic']\n",
        "    if topic_num != -1:\n",
        "        topic_name = row['Name']\n",
        "        pure_topic_name = \"_\".join(topic_name.split(\"_\")[1:])\n",
        "        lst.append(pure_topic_name)\n",
        "for i in lst:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "dMZIXrc1G3hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
        "bertopic_model_embed_token = BERTopic(embedding_model=model,\n",
        "                                      vectorizer_model=vectorizer,\n",
        "                                      ctfidf_model=ctfidf_model,\n",
        "                                      nr_topics=100,\n",
        "                                      top_n_words=3,\n",
        "                                      calculate_probabilities=True)\n",
        "topics, _ = bertopic_model_embed_token.fit_transform(cocktail)"
      ],
      "metadata": {
        "id": "7Uruk1MXQjY-"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_info = bertopic_model_embed_token.get_topic_info()\n",
        "number_of_topics = len(topic_info) - 1\n",
        "print(f\"생성된 주제의 수: {number_of_topics}\")\n",
        "lst = []\n",
        "for index, row in topic_info.iterrows():\n",
        "    topic_num = row['Topic']\n",
        "    if topic_num != -1:\n",
        "        topic_name = row['Name']\n",
        "        pure_topic_name = \"_\".join(topic_name.split(\"_\")[1:])\n",
        "        lst.append(pure_topic_name)\n",
        "for i in lst:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "I7QOVUY2QuUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}